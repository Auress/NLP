{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Урок 3. Part-of-Speech разметка, NER , извлечение отношений#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### -- Автор: Шенк Евгений Станиславович"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Тема “Улучшение качества  POS-taggera”\n",
    "на вебинаре был рассмотрен пример тегирования с использованием корпуса на Русском языке, вам необходимо улучшить модель что бы качество классификации было выше чем с лог регрессией "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import brown\n",
    "from nltk.tag import DefaultTagger\n",
    "from nltk.tag import UnigramTagger\n",
    "from nltk.tag import BigramTagger, TrigramTagger\n",
    "from nltk.tag import RegexpTagger\n",
    "\n",
    "import pyconll\n",
    "from sklearn.feature_extraction.text import CountVectorizer, HashingVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgbm\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "\n",
    "# url = 'https://raw.githubusercontent.com/UniversalDependencies/UD_Russian-SynTagRus/master/ru_syntagrus-ud-train.conllu'\n",
    "# urllib.request.urlretrieve(url, './datasets/ru_syntagrus-ud-train.conllu')\n",
    "\n",
    "# url = 'https://raw.githubusercontent.com/UniversalDependencies/UD_Russian-SynTagRus/master/ru_syntagrus-ud-dev.conllu'\n",
    "# urllib.request.urlretrieve(url, './datasets/ru_syntagrus-ud-dev.conllu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train-Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_train = pyconll.load_from_file('datasets/ru_syntagrus-ud-train.conllu')\n",
    "full_test = pyconll.load_from_file('datasets/ru_syntagrus-ud-dev.conllu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdata_train = []\n",
    "for sent in full_train[:]:\n",
    "    fdata_train.append([(token.form, token.upos) for token in sent])\n",
    "    \n",
    "fdata_test = []\n",
    "for sent in full_test[:]:\n",
    "    fdata_test.append([(token.form, token.upos) for token in sent])\n",
    "    \n",
    "fdata_sent_test = []\n",
    "for sent in full_test[:]:\n",
    "    fdata_sent_test.append([token.form for token in sent])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tok = []\n",
    "train_label = []\n",
    "for sent in fdata_train[:]:\n",
    "    for tok in sent:\n",
    "        train_tok.append(tok[0])\n",
    "        train_label.append('NO_TAG' if tok[1] is None else tok[1])\n",
    "        \n",
    "test_tok = []\n",
    "test_label = []\n",
    "for sent in fdata_test[:]:\n",
    "    for tok in sent:\n",
    "        test_tok.append(tok[0])\n",
    "        test_label.append('NO_TAG' if tok[1] is None else tok[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "train_enc_labels = le.fit_transform(train_label)\n",
    "test_enc_labels = le.transform(test_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "hvectorizer = HashingVectorizer(ngram_range=(1, 3), analyzer='char', n_features=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = hvectorizer.fit_transform(train_tok)\n",
    "X_test = hvectorizer.transform(test_tok)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(max_iter=10, random_state=0)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LogisticRegression(random_state=0, max_iter=10)\n",
    "lr.fit(X_train, train_enc_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_lr = lr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5258484143834462"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(test_enc_labels, pred_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bigram tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8829996966939642"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "unigram_tagger = UnigramTagger(fdata_train)\n",
    "bigram_tagger = BigramTagger(fdata_train, backoff=unigram_tagger)\n",
    "display(bigram_tagger.evaluate(fdata_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model = xgb.XGBClassifier(learning_rate=0.05, \n",
    "                              max_depth=15,\n",
    "                              n_estimators=200, \n",
    "                              nthread=-1,\n",
    "                              objective='binary:logistic',\n",
    "                              reg_alpha=0,\n",
    "                              reg_lambda=0,\n",
    "                              random_state=0, \n",
    "                              subsample=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster=None, colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
       "              importance_type='gain', interaction_constraints=None,\n",
       "              learning_rate=0.05, max_delta_step=0, max_depth=15,\n",
       "              min_child_weight=1, missing=nan, monotone_constraints=None,\n",
       "              n_estimators=200, n_jobs=-1, nthread=-1, num_parallel_tree=1,\n",
       "              objective='multi:softprob', random_state=0, reg_alpha=0,\n",
       "              reg_lambda=0, scale_pos_weight=None, subsample=1,\n",
       "              tree_method=None, validate_parameters=False, verbosity=None)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_model.fit(X_train, train_enc_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_xgb = xgb_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9423718531998787"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(test_enc_labels, pred_xgb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Лучший результат у XGBoost (0.942)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Тема “Создание признакового пространства”"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Задание 1 и 2\n",
    "Используя библиотеку Spacy, вывести ТОП-20 популярных NER в combine_df датасете. Какой тип NER (ORG, GPE, PERSON и тд) оказался самым популярным? (Учтите, что max_word_limit_spacy для Spacy = 1000000)  \n",
    "С помощью Spacy выяснить: какие персоны и организации самые обсуждаемые в train и test датасетах? вывести ТОП-20 самых популярных. Действительно ли в топ вошли только персоны и организации или есть мусор?  \n",
    "Повторим шаги из заданий 1 и 2, используя библиотеку nltk.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "import en_core_web_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "      <th>clean_tweet</th>\n",
       "      <th>tweet_token</th>\n",
       "      <th>tweet_token_filtered</th>\n",
       "      <th>tweet_stemmed</th>\n",
       "      <th>tweet_lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>@user when a father is dysfunctional and is s...</td>\n",
       "      <td>when father is dysfunctional and is so selfish...</td>\n",
       "      <td>[when, father, is, dysfunctional, and, is, so,...</td>\n",
       "      <td>[father, dysfunctional, selfish, drags, kids, ...</td>\n",
       "      <td>[father, dysfunct, selfish, drag, kid, dysfunc...</td>\n",
       "      <td>[father, dysfunctional, selfish, drag, kid, dy...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>@user @user thanks for #lyft credit i can't us...</td>\n",
       "      <td>thanks for lyft credit cannot use cause they d...</td>\n",
       "      <td>[thanks, for, lyft, credit, can, not, use, cau...</td>\n",
       "      <td>[thanks, lyft, credit, use, cause, offer, whee...</td>\n",
       "      <td>[thank, lyft, credit, use, caus, offer, wheelc...</td>\n",
       "      <td>[thanks, lyft, credit, use, cause, offer, whee...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  label                                              tweet  \\\n",
       "0   1    0.0   @user when a father is dysfunctional and is s...   \n",
       "1   2    0.0  @user @user thanks for #lyft credit i can't us...   \n",
       "\n",
       "                                         clean_tweet  \\\n",
       "0  when father is dysfunctional and is so selfish...   \n",
       "1  thanks for lyft credit cannot use cause they d...   \n",
       "\n",
       "                                         tweet_token  \\\n",
       "0  [when, father, is, dysfunctional, and, is, so,...   \n",
       "1  [thanks, for, lyft, credit, can, not, use, cau...   \n",
       "\n",
       "                                tweet_token_filtered  \\\n",
       "0  [father, dysfunctional, selfish, drags, kids, ...   \n",
       "1  [thanks, lyft, credit, use, cause, offer, whee...   \n",
       "\n",
       "                                       tweet_stemmed  \\\n",
       "0  [father, dysfunct, selfish, drag, kid, dysfunc...   \n",
       "1  [thank, lyft, credit, use, caus, offer, wheelc...   \n",
       "\n",
       "                                    tweet_lemmatized  \n",
       "0  [father, dysfunctional, selfish, drag, kid, dy...  \n",
       "1  [thanks, lyft, credit, use, cause, offer, whee...  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_pickle(\"../data/tweet_dataframe.pkl\")\n",
    "data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train, Test = train_test_split(data, train_size=0.7, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text = \"\".join(str(x) for x in Train['clean_tweet'])\n",
    "test_text = \"\".join(str(x) for x in Test['clean_tweet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\".join(str(x) for x in data['clean_tweet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = en_core_web_md.load()\n",
    "nlp.max_length = 4000000\n",
    "#doc = nlp(\"I think Barack Obama met founder of Facebook at occasion of a release of a new NLP algorithm.\")\n",
    "doc = nlp(text)\n",
    "\n",
    "nlp_train = en_core_web_md.load()\n",
    "nlp_train.max_length = 4000000\n",
    "doc_train = nlp_train(train_text)\n",
    "\n",
    "nlp_test = en_core_web_md.load()\n",
    "nlp_test.max_length = 4000000\n",
    "doc_test = nlp_test(test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Отображение (закоменченно т.к. сильно увеличивает размер нотбука)\n",
    "# displacy.render(doc, style=\"ent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "ents_df = pd.DataFrame(columns=['ents', 'label'])\n",
    "for i, token in enumerate(doc.ents):\n",
    "    ents_df.loc[i] = [str(token), str(token.label_)]\n",
    "    \n",
    "ents_df_train = pd.DataFrame(columns=['ents', 'label'])\n",
    "for i, token in enumerate(doc_train.ents):\n",
    "    ents_df_train.loc[i] = [str(token), str(token.label_)]\n",
    "    \n",
    "ents_df_test = pd.DataFrame(columns=['ents', 'label'])\n",
    "for i, token in enumerate(doc_test.ents):\n",
    "    ents_df_test.loc[i] = [str(token), str(token.label_)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ents\n",
       "today       1599\n",
       "friday       619\n",
       "first        612\n",
       "tomorrow     576\n",
       "one          533\n",
       "sunday       489\n",
       "tonight      451\n",
       "orlando      428\n",
       "summer       419\n",
       "saturday     319\n",
       "monday       241\n",
       "morning      230\n",
       "america      198\n",
       "two          195\n",
       "weekend      181\n",
       "june         180\n",
       "us           179\n",
       "london       176\n",
       "thursday     167\n",
       "days         142\n",
       "Name: ents, dtype: int64"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ents_df['ents'].groupby(by=ents_df['ents']).count().sort_values(ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ents\n",
       "obama                  108\n",
       "gop                     85\n",
       "hu                      68\n",
       "hillary                 68\n",
       "nba                     66\n",
       "orlando                 56\n",
       "bing bong bing bong     52\n",
       "disney                  49\n",
       "bing bong               45\n",
       "isis                    43\n",
       "stas                    42\n",
       "blm                     39\n",
       "lebron                  38\n",
       "netflix                 38\n",
       "christina grimmie       35\n",
       "donald                  32\n",
       "jesus                   29\n",
       "nyc                     28\n",
       "amazon                  28\n",
       "cavs                    27\n",
       "Name: ents, dtype: int64"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Total\n",
    "ents_org_person = ents_df[ents_df['label'].isin(['ORG', \"PERSON\"])]\n",
    "ents_org_person['ents'].groupby(by=ents_df['ents']).count().sort_values(ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ents\n",
       "obama                  86\n",
       "orlando                55\n",
       "hu                     54\n",
       "gop                    51\n",
       "hillary                46\n",
       "bing bong bing bong    39\n",
       "bing bong              35\n",
       "stas                   35\n",
       "netflix                34\n",
       "isis                   31\n",
       "christina grimmie      29\n",
       "disney                 28\n",
       "nba                    27\n",
       "lebron                 27\n",
       "nyc                    25\n",
       "jesus                  24\n",
       "blm                    23\n",
       "sun                    23\n",
       "islam                  21\n",
       "god                    20\n",
       "Name: ents, dtype: int64"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train\n",
    "ents_org_person = ents_df_train[ents_df_train['label'].isin(['ORG', \"PERSON\"])]\n",
    "ents_org_person['ents'].groupby(by=ents_org_person['ents']).count().sort_values(ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ents\n",
       "obama                  30\n",
       "nba                    29\n",
       "gop                    24\n",
       "hillary                24\n",
       "orlando                19\n",
       "disney                 17\n",
       "bing bong bing bong    16\n",
       "hu                     15\n",
       "bing bong              14\n",
       "cnn                    13\n",
       "donald                 12\n",
       "cavs                   12\n",
       "blm                    12\n",
       "nyc                    10\n",
       "isis                   10\n",
       "aap                     9\n",
       "eu                      9\n",
       "jo cox                  8\n",
       "fbi                     8\n",
       "islam                   8\n",
       "Name: ents, dtype: int64"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test\n",
    "ents_org_person = ents_df_test[ents_df_test['label'].isin(['ORG', \"PERSON\"])]\n",
    "ents_org_person['ents'].groupby(by=ents_org_person['ents']).count().sort_values(ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### В топ попадают не только персоны и организации но и \"bing bong\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_of_ents = {(' '.join(c[0] for c in chunk), chunk.label() ) for chunk in nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(text.upper()))) if hasattr(chunk, 'label') }\n",
    "ents_df_nltk = pd.DataFrame(set_of_ents, columns=['ents', 'label'])\n",
    "\n",
    "set_of_ents = {(' '.join(c[0] for c in chunk), chunk.label() ) for chunk in nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(train_text.upper()))) if hasattr(chunk, 'label') }\n",
    "ents_df_nltk_train = pd.DataFrame(set_of_ents, columns=['ents', 'label'])\n",
    "\n",
    "set_of_ents = {(' '.join(c[0] for c in chunk), chunk.label() ) for chunk in nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(test_text.upper()))) if hasattr(chunk, 'label') }\n",
    "ents_df_nltk_test = pd.DataFrame(set_of_ents, columns=['ents', 'label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ents</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>INEEDIT</td>\n",
       "      <td>ORGANIZATION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>REN</td>\n",
       "      <td>ORGANIZATION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PETAL</td>\n",
       "      <td>ORGANIZATION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BLACKMAGIC</td>\n",
       "      <td>ORGANIZATION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>WISHES</td>\n",
       "      <td>ORGANIZATION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6215</th>\n",
       "      <td>DNC</td>\n",
       "      <td>ORGANIZATION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6216</th>\n",
       "      <td>MOVEMENT</td>\n",
       "      <td>ORGANIZATION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6217</th>\n",
       "      <td>HIRE</td>\n",
       "      <td>ORGANIZATION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6218</th>\n",
       "      <td>FURRBABY</td>\n",
       "      <td>ORGANIZATION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6219</th>\n",
       "      <td>NIGHTCLUB</td>\n",
       "      <td>ORGANIZATION</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6220 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            ents         label\n",
       "0        INEEDIT  ORGANIZATION\n",
       "1            REN  ORGANIZATION\n",
       "2          PETAL  ORGANIZATION\n",
       "3     BLACKMAGIC  ORGANIZATION\n",
       "4         WISHES  ORGANIZATION\n",
       "...          ...           ...\n",
       "6215         DNC  ORGANIZATION\n",
       "6216    MOVEMENT  ORGANIZATION\n",
       "6217        HIRE  ORGANIZATION\n",
       "6218    FURRBABY  ORGANIZATION\n",
       "6219   NIGHTCLUB  ORGANIZATION\n",
       "\n",
       "[6220 rows x 2 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ents_df_nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ents\n",
       "HENDERSON      2\n",
       "CLIENTS        2\n",
       "IRELAND        2\n",
       "WHITE          2\n",
       "CHINA          2\n",
       "US             2\n",
       "JOHNSON        2\n",
       "TODDLER        2\n",
       "ISRAEL         2\n",
       "FARRAH         1\n",
       "FANS OF        1\n",
       "FAN OF         1\n",
       "FATAL          1\n",
       "FAT            1\n",
       "FANEPICNESS    1\n",
       "FASHION        1\n",
       "FANG           1\n",
       "FASCIST        1\n",
       "FANS           1\n",
       "FANTASTIC      1\n",
       "Name: ents, dtype: int64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ents_df_nltk['ents'].groupby(by=ents_df_nltk['ents']).count().sort_values(ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ents\n",
       "HENDERSON        2\n",
       "TODDLER          2\n",
       "JOHNSON          2\n",
       "CLIENTS          2\n",
       "ZUMA             1\n",
       "FARHAD           1\n",
       "FANEPICNESS      1\n",
       "FANG             1\n",
       "FANS             1\n",
       "FANS OF          1\n",
       "FANTASTIC        1\n",
       "FAR              1\n",
       "FAREWELL         1\n",
       "FARIDOON         1\n",
       "FAMOUS FOR       1\n",
       "FARMERS          1\n",
       "FARMERSMARKET    1\n",
       "FARMHOUSE        1\n",
       "FARRAH           1\n",
       "FASCISM          1\n",
       "Name: ents, dtype: int64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Total\n",
    "ents_nltk_org_person = ents_df_nltk[ents_df_nltk['label'].isin(['ORGANIZATION', \"PERSON\"])]\n",
    "ents_nltk_org_person['ents'].groupby(by=ents_nltk_org_person['ents']).count().sort_values(ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ents\n",
       "WORSE TRAGEDIES        1\n",
       "CLINTON NOMINEE        1\n",
       "CLINTON MAKES ME       1\n",
       "CLINTON JIMMY          1\n",
       "CLINTON IT             1\n",
       "CLINTON INSPIRED       1\n",
       "CLINTON HATES          1\n",
       "CLINTON FOR            1\n",
       "CLINTON DEFENDED       1\n",
       "CLINTON BLACK          1\n",
       "CLINTON ARE            1\n",
       "CLINTON AM             1\n",
       "CLINTON                1\n",
       "CLINE AK               1\n",
       "CLIFFHANGER FCKU       1\n",
       "CLIFF HOW              1\n",
       "CLICK TO WATCH LOTS    1\n",
       "CLINTON MY             1\n",
       "CLINTON SAY            1\n",
       "WOODROWWILSON HELD     1\n",
       "Name: ents, dtype: int64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train\n",
    "ents_org_person = ents_df_nltk_train[ents_df_nltk_train['label'].isin(['ORG', \"PERSON\"])]\n",
    "ents_org_person['ents'].groupby(by=ents_org_person['ents']).count().sort_values(ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ents\n",
       "WILLIAM AND          1\n",
       "TODDLER              1\n",
       "IMWITHHER BE         1\n",
       "HENDERSON            1\n",
       "HAPPY MOOD           1\n",
       "GARRETT HAS          1\n",
       "ELECT                1\n",
       "DONALD TRUMP         1\n",
       "DAVID PROPER         1\n",
       "CLINTON RALLY        1\n",
       "CLINTON MADE         1\n",
       "CLINTON HE           1\n",
       "CLINTON              1\n",
       "CLIFTON VIDEO MEN    1\n",
       "CLIENTS              1\n",
       "CLICK TO WATCH AM    1\n",
       "CLICK TO WATCH       1\n",
       "CLICK TO DOWNLOAD    1\n",
       "CLICK TO             1\n",
       "CLICK SHOULD         1\n",
       "Name: ents, dtype: int64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test\n",
    "ents_org_person = ents_df_nltk_test[ents_df_nltk_test['label'].isin(['ORG', \"PERSON\"])]\n",
    "ents_org_person['ents'].groupby(by=ents_org_person['ents']).count().sort_values(ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 3.\n",
    "Какая из библиотек по вашему лучше отработала? Сравните качество полученных most_common NER и количество распознаных NER."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33791"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Распознано Spacy:  # 33791\n",
    "ents_df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ents\n",
       "today       1599\n",
       "friday       619\n",
       "first        612\n",
       "tomorrow     576\n",
       "one          533\n",
       "sunday       489\n",
       "tonight      451\n",
       "orlando      428\n",
       "summer       419\n",
       "saturday     319\n",
       "monday       241\n",
       "morning      230\n",
       "america      198\n",
       "two          195\n",
       "weekend      181\n",
       "june         180\n",
       "us           179\n",
       "london       176\n",
       "thursday     167\n",
       "days         142\n",
       "Name: ents, dtype: int64"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ents_df['ents'].groupby(by=ents_df['ents']).count().sort_values(ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13668"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Распознано NLTK:  # 13668\n",
    "ents_df_nltk.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ents\n",
       "AMERICANS    2\n",
       "FATHER       2\n",
       "TRUMP        2\n",
       "PERSON       2\n",
       "MADISON      2\n",
       "IRAQ         2\n",
       "IRELAND      2\n",
       "OBAMA        2\n",
       "CLINTON      2\n",
       "BELGIAN      2\n",
       "US           2\n",
       "ALBUM        2\n",
       "JOHN         2\n",
       "CLIENTS      2\n",
       "TURKEY       2\n",
       "BERNIE       2\n",
       "BARRY        2\n",
       "INDIAN       2\n",
       "ISRAEL       2\n",
       "JOHNSON      2\n",
       "Name: ents, dtype: int64"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ents_df_nltk['ents'].groupby(by=ents_df_nltk['ents']).count().sort_values(ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Качество Spacy оказалось лучше, количество распознаваний выше, релевантность выше, отработал быстрее. В NLTK пришлось сделать весь текст заглавными буквами, иначе результата не было вовсе."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
