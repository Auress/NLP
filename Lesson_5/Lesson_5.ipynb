{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Урок 5. Сверточные нейронные сети для анализа текста.#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### -- Автор: Шенк Евгений Станиславович"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import keras\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Input, Embedding, Conv1D, GlobalMaxPool1D, MaxPooling1D, BatchNormalization, Masking, InputLayer\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.callbacks import TensorBoard\n",
    "from tensorflow.keras import optimizers\n",
    "from keras.objectives import categorical_crossentropy\n",
    "from tensorflow.keras.callbacks import EarlyStopping  \n",
    "from gensim.models import Word2Vec, FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = tf.device('cpu')  #'cpu' 'gpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Выполнение:   \n",
    "Максимальный roc_auc_score который удалось получить на валидационной выборке = 0.87.  \n",
    "Обучение на эмбедингах Word2Vec дает похожий результат, но при обучении эмбедингов (модели M2V) на изначальных твитах (без пред обработки), при обучении на обработанных данных качество сильно падало. FastText отработал чуть хуже W2V."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Задание 1. Учим conv сеть для классификации - выбить auc выше 0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words = 50000\n",
    "max_len = 50\n",
    "num_classes = 1\n",
    "\n",
    "# Training\n",
    "epochs = 50\n",
    "batch_size = 64  #64\n",
    "print_batch_n = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"../data/train.csv\")\n",
    "df_test = pd.read_csv(\"../data/test.csv\")\n",
    "df_val = pd.read_csv(\"../data/val.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qEyvOxoOJDTP"
   },
   "source": [
    "### Предобработка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "from stop_words import get_stop_words\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sw = set(get_stop_words(\"ru\"))\n",
    "exclude = set(punctuation)\n",
    "morpher = MorphAnalyzer()\n",
    "\n",
    "def preprocess_text(txt):\n",
    "    txt = str(txt)\n",
    "    txt = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))', 'URL', txt) # заменяем URL\n",
    "    txt = re.sub('@[^\\s]+', 'USER', txt) # заменяем username\n",
    "    txt = \"\".join(c if c not in exclude else \" \" for c in txt) # убираем пунктуацию\n",
    "    txt = txt.lower()\n",
    "    txt = re.sub(\"не\\s\", \"не\", txt)\n",
    "    txt = re.sub(\"\\s{2,}\", \" \", txt) # убираем лишние пробелы\n",
    "    # Не удаляем стоп-слова, т.к. качество падает (в некоторых твитах все слова оказываются стоп-словами)\n",
    "    txt = [morpher.parse(word)[0].normal_form for word in txt.split()]  # if word not in sw \n",
    "    return \" \".join(txt)\n",
    "\n",
    "df_train['text'] = df_train['text'].apply(preprocess_text)\n",
    "df_val['text'] = df_val['text'].apply(preprocess_text)\n",
    "df_test['text'] = df_test['text'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_train.to_csv(\"../data/train_2.csv\", index=False)\n",
    "#df_val.to_csv(\"../data/val_2.csv\", index=False)\n",
    "#df_test.to_csv(\"../data/test_2.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C4o9QgmWI3Pw"
   },
   "outputs": [],
   "source": [
    "#df_train.fillna('', inplace=True)\n",
    "#df_val.fillna('', inplace=True)\n",
    "#df_test.fillna('', inplace=True)\n",
    "\n",
    "train_corpus = \" \".join(df_train[\"text\"])\n",
    "train_corpus = train_corpus.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 10183,
     "status": "ok",
     "timestamp": 1590650264825,
     "user": {
      "displayName": "Roman Zakharov",
      "photoUrl": "",
      "userId": "18255168926005506833"
     },
     "user_tz": -240
    },
    "id": "Hed2ySbwJH6B",
    "outputId": "66a75988-b4e3-45aa-a483-adbdc766db03"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, TweetTokenizer\n",
    "# nltk.download(\"punkt\")\n",
    "\n",
    "# tokens = word_tokenize(train_corpus)\n",
    "tweet_tokenizer = TweetTokenizer()  # Используем TweetTokenizer, но на качество по-моему не влияет\n",
    "tokens = tweet_tokenizer.tokenize(train_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yJ8T0fwYJYJX"
   },
   "source": [
    "Отфильтруем данные\n",
    "\n",
    "и соберём в корпус N наиболее частых токенов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EXOLVK1tJLT8"
   },
   "outputs": [],
   "source": [
    "tokens_filtered = [word for word in tokens if word.isalnum()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8qCQH5nIJoiB"
   },
   "outputs": [],
   "source": [
    "from nltk.probability import FreqDist\n",
    "dist = FreqDist(tokens_filtered)\n",
    "tokens_filtered_top = [pair[0] for pair in dist.most_common(max_words-1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 340
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 576,
     "status": "ok",
     "timestamp": 1590650396521,
     "user": {
      "displayName": "Roman Zakharov",
      "photoUrl": "",
      "userId": "18255168926005506833"
     },
     "user_tz": -240
    },
    "id": "bRQ-6wwjJrGo",
    "outputId": "ba8ebf9e-c6aa-4703-b54f-39b561e33493"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['user', 'я', 'и', 'в', 'что', 'rt', 'на', 'а', 'url', 'с']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_filtered_top[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Tdk777qGJtz4"
   },
   "outputs": [],
   "source": [
    "vocabulary = {v: k for k, v in dict(enumerate(tokens_filtered_top, 1)).items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5OULZgvkJzpj"
   },
   "outputs": [],
   "source": [
    "def text_to_sequence(text, maxlen):\n",
    "    result = []\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    tokens_filtered = [word for word in tokens if word.isalnum()]\n",
    "    for word in tokens_filtered:\n",
    "        if word in vocabulary:\n",
    "            result.append(vocabulary[word])\n",
    "    padding = [0]*(maxlen-len(result))\n",
    "    return padding + result[-maxlen:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pqHlf5nNJ2hl"
   },
   "outputs": [],
   "source": [
    "x_train = np.asarray([text_to_sequence(text, max_len) for text in df_train[\"text\"]], dtype=np.int32)\n",
    "x_test = np.asarray([text_to_sequence(text, max_len) for text in df_test[\"text\"]], dtype=np.int32)\n",
    "x_val = np.asarray([text_to_sequence(text, max_len) for text in df_val[\"text\"]], dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 562,
     "status": "ok",
     "timestamp": 1590650625870,
     "user": {
      "displayName": "Roman Zakharov",
      "photoUrl": "",
      "userId": "18255168926005506833"
     },
     "user_tz": -240
    },
    "id": "lI4NUg_TJ6NK",
    "outputId": "5d29285d-6c2e-4c34-f307-c2e9c3c12631"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(181467, 50)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 643,
     "status": "ok",
     "timestamp": 1590650628713,
     "user": {
      "displayName": "Roman Zakharov",
      "photoUrl": "",
      "userId": "18255168926005506833"
     },
     "user_tz": -240
    },
    "id": "9QlLvXd9KDf3",
    "outputId": "0355c6dc-b479-4031-bf7d-54f158607906"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     6,     1,\n",
       "         304,     3,  7078,    16,     4,   572,    32,    13,   254,\n",
       "          41,    63,     3,   119, 29466])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 2\n",
    "y_train = keras.utils.to_categorical(df_train[\"class\"], num_classes)\n",
    "y_val = keras.utils.to_categorical(df_val[\"class\"], num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=max_words, output_dim=64, input_length=max_len))\n",
    "\n",
    "model.add(Conv1D(64, 3))  # 128\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(GlobalMaxPool1D())\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(32))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(Dense(16))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(Dense(num_classes))\n",
    "model.add(Activation('softmax')) # sigmoid  softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "adam = optimizers.Adam(lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', # loss='binary_crossentropy'  loss='categorical_crossentropy'\n",
    "              optimizer=adam,\n",
    "              metrics=['AUC'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "   1/2269 [..............................] - ETA: 0s - loss: 0.6949 - auc: 0.4890WARNING:tensorflow:From C:\\Users\\Eugene\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\summary_ops_v2.py:1277: stop (from tensorflow.python.eager.profiler) is deprecated and will be removed after 2020-07-01.\n",
      "Instructions for updating:\n",
      "use `tf.profiler.experimental.stop` instead.\n",
      "2269/2269 [==============================] - 54s 24ms/step - loss: 0.5729 - auc: 0.7670 - val_loss: 0.4808 - val_auc: 0.8480\n",
      "Epoch 2/50\n",
      "2269/2269 [==============================] - 54s 24ms/step - loss: 0.4402 - auc: 0.8764 - val_loss: 0.4571 - val_auc: 0.8645\n",
      "Epoch 3/50\n",
      "2269/2269 [==============================] - 54s 24ms/step - loss: 0.3830 - auc: 0.9091 - val_loss: 0.4601 - val_auc: 0.8664\n"
     ]
    }
   ],
   "source": [
    "with device:\n",
    "    tensorboard=TensorBoard(log_dir='./logs', write_graph=True, write_images=True)\n",
    "    early_stopping=EarlyStopping(monitor='val_loss')  \n",
    "\n",
    "\n",
    "    history = model.fit(x_train, y_train,\n",
    "                        batch_size=batch_size,\n",
    "                        epochs=epochs,\n",
    "                        verbose=1,\n",
    "                        validation_split=0.2,\n",
    "                        callbacks=[tensorboard, early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "355/355 [==============================] - 1s 2ms/step - loss: 0.4632 - auc: 0.8642\n",
      "\n",
      "\n",
      "Test score: 0.46322160959243774\n",
      "Test accuracy: 0.8642386198043823\n"
     ]
    }
   ],
   "source": [
    "with device:\n",
    "    score = model.evaluate(x_val, y_val, batch_size=batch_size, verbose=1)\n",
    "print('\\n')\n",
    "print('Test score:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device('cpu'):\n",
    "    result = model.predict(x_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8642297362020667"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(y_val, result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.8642297362020667"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 2. Предобучаем word2vec и его эмбединга инициализируем сетку, как влияет на качество?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"../data/train.csv\")\n",
    "df_test = pd.read_csv(\"../data/test.csv\")\n",
    "df_val = pd.read_csv(\"../data/val.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 2\n",
    "y_train = keras.utils.to_categorical(df_train[\"class\"], num_classes)\n",
    "y_val = keras.utils.to_categorical(df_val[\"class\"], num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = df_train[\"text\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 300\n",
    "modelW2V = Word2Vec(sentences=sentences, size=size, window=5, min_count=1, workers= 32, seed = 34)\n",
    "#modelW2V = FastText(sentences=sentences, size=300, window=5, min_count=1, workers= 32, seed = 34)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(101477133, 294208920)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelW2V.train(sentences=sentences, total_examples=len(df_train[\"text\"]), epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_to_vec(model, sentence, size):\n",
    "    size = size\n",
    "    vec = np.zeros(size)\n",
    "    vec_len = 0\n",
    "\n",
    "    for word in sentence:\n",
    "        try:  # Бывают слова которых нет в словаре\n",
    "            vec += model[word].reshape((1, size))[0]\n",
    "            vec_len += 1\n",
    "        except KeyError:\n",
    "            pass\n",
    "\n",
    "    result = vec / vec_len\n",
    "\n",
    "    return result    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Eugene\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:8: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "xtrain_w2v = df_train[\"text\"].apply(lambda x: sentence_to_vec(modelW2V, x, size))\n",
    "xtrain_w2v = np.array(xtrain_w2v.tolist())\n",
    "xvalid_w2v = df_val['text'].apply(lambda x: sentence_to_vec(modelW2V, x, size))\n",
    "xvalid_w2v = np.array(xvalid_w2v.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain_w2v = xtrain_w2v.reshape(len(xtrain_w2v), size, 1)\n",
    "xvalid_w2v = xvalid_w2v.reshape(len(xvalid_w2v), size, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_w2v = Sequential()\n",
    "\n",
    "model_w2v.add(Conv1D(64, 3))  # 128\n",
    "model_w2v.add(Activation(\"relu\"))\n",
    "model_w2v.add(GlobalMaxPool1D())\n",
    "model_w2v.add(Dropout(0.1))\n",
    "model_w2v.add(Dense(32))\n",
    "model_w2v.add(Activation(\"relu\"))\n",
    "model_w2v.add(Dense(16))\n",
    "model_w2v.add(Activation(\"relu\"))\n",
    "model_w2v.add(Dense(num_classes))\n",
    "model_w2v.add(Activation('softmax')) # sigmoid  softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "adam = optimizers.Adam(lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_w2v.compile(loss='categorical_crossentropy', # loss='binary_crossentropy'  loss='categorical_crossentropy'\n",
    "              optimizer=adam,\n",
    "              metrics=['AUC'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "   2/2269 [..............................] - ETA: 16:36 - loss: 0.6902 - auc: 0.5905WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0070s vs `on_train_batch_end` time: 0.8712s). Check your callbacks.\n",
      "2269/2269 [==============================] - 19s 8ms/step - loss: 0.6783 - auc: 0.6047 - val_loss: 0.6685 - val_auc: 0.6383\n",
      "Epoch 2/50\n",
      "2269/2269 [==============================] - 16s 7ms/step - loss: 0.6670 - auc: 0.6308 - val_loss: 0.6621 - val_auc: 0.6539\n",
      "Epoch 3/50\n",
      "2269/2269 [==============================] - 16s 7ms/step - loss: 0.6630 - auc: 0.6396 - val_loss: 0.6563 - val_auc: 0.6671\n",
      "Epoch 4/50\n",
      "2269/2269 [==============================] - 15s 7ms/step - loss: 0.6594 - auc: 0.6465 - val_loss: 0.6504 - val_auc: 0.6802\n",
      "Epoch 5/50\n",
      "2269/2269 [==============================] - 15s 7ms/step - loss: 0.6563 - auc: 0.6532 - val_loss: 0.6456 - val_auc: 0.6840\n",
      "Epoch 6/50\n",
      "2269/2269 [==============================] - 15s 7ms/step - loss: 0.6526 - auc: 0.6596 - val_loss: 0.6412 - val_auc: 0.6963\n",
      "Epoch 7/50\n",
      "2269/2269 [==============================] - 15s 7ms/step - loss: 0.6496 - auc: 0.6653 - val_loss: 0.6343 - val_auc: 0.7083\n",
      "Epoch 8/50\n",
      "2269/2269 [==============================] - 15s 7ms/step - loss: 0.6462 - auc: 0.6709 - val_loss: 0.6302 - val_auc: 0.7195\n",
      "Epoch 9/50\n",
      "2269/2269 [==============================] - 15s 7ms/step - loss: 0.6415 - auc: 0.6794 - val_loss: 0.6218 - val_auc: 0.7216\n",
      "Epoch 10/50\n",
      "2269/2269 [==============================] - 15s 7ms/step - loss: 0.6352 - auc: 0.6900 - val_loss: 0.6119 - val_auc: 0.7387\n",
      "Epoch 11/50\n",
      "2269/2269 [==============================] - 15s 7ms/step - loss: 0.6295 - auc: 0.6998 - val_loss: 0.6048 - val_auc: 0.7412\n",
      "Epoch 12/50\n",
      "2269/2269 [==============================] - 15s 7ms/step - loss: 0.6222 - auc: 0.7104 - val_loss: 0.5937 - val_auc: 0.7639\n",
      "Epoch 13/50\n",
      "2269/2269 [==============================] - 15s 7ms/step - loss: 0.6155 - auc: 0.7190 - val_loss: 0.5857 - val_auc: 0.7690\n",
      "Epoch 14/50\n",
      "2269/2269 [==============================] - 15s 7ms/step - loss: 0.6092 - auc: 0.7273 - val_loss: 0.5793 - val_auc: 0.7859\n",
      "Epoch 15/50\n",
      "2269/2269 [==============================] - 15s 7ms/step - loss: 0.6031 - auc: 0.7346 - val_loss: 0.5686 - val_auc: 0.7859\n",
      "Epoch 16/50\n",
      "2269/2269 [==============================] - 15s 7ms/step - loss: 0.5942 - auc: 0.7443 - val_loss: 0.5599 - val_auc: 0.7933\n",
      "Epoch 17/50\n",
      "2269/2269 [==============================] - 15s 7ms/step - loss: 0.5876 - auc: 0.7517 - val_loss: 0.5554 - val_auc: 0.8029\n",
      "Epoch 18/50\n",
      "2269/2269 [==============================] - 15s 7ms/step - loss: 0.5809 - auc: 0.7586 - val_loss: 0.5480 - val_auc: 0.8006\n",
      "Epoch 19/50\n",
      "2269/2269 [==============================] - 15s 7ms/step - loss: 0.5732 - auc: 0.7660 - val_loss: 0.5407 - val_auc: 0.8145\n",
      "Epoch 20/50\n",
      "2269/2269 [==============================] - 15s 7ms/step - loss: 0.5662 - auc: 0.7731 - val_loss: 0.5353 - val_auc: 0.8143\n",
      "Epoch 21/50\n",
      "2269/2269 [==============================] - 16s 7ms/step - loss: 0.5588 - auc: 0.7802 - val_loss: 0.5287 - val_auc: 0.8210\n",
      "Epoch 22/50\n",
      "2269/2269 [==============================] - 17s 7ms/step - loss: 0.5533 - auc: 0.7850 - val_loss: 0.5216 - val_auc: 0.8179\n",
      "Epoch 23/50\n",
      "2269/2269 [==============================] - 17s 7ms/step - loss: 0.5495 - auc: 0.7885 - val_loss: 0.5169 - val_auc: 0.8222\n",
      "Epoch 24/50\n",
      "2269/2269 [==============================] - 17s 7ms/step - loss: 0.5449 - auc: 0.7924 - val_loss: 0.5124 - val_auc: 0.8313\n",
      "Epoch 25/50\n",
      "2269/2269 [==============================] - 16s 7ms/step - loss: 0.5401 - auc: 0.7969 - val_loss: 0.5064 - val_auc: 0.8375\n",
      "Epoch 26/50\n",
      "2269/2269 [==============================] - 17s 7ms/step - loss: 0.5359 - auc: 0.8002 - val_loss: 0.5084 - val_auc: 0.8296\n"
     ]
    }
   ],
   "source": [
    "with device:\n",
    "    tensorboard=TensorBoard(log_dir='./logs', write_graph=True, write_images=True)\n",
    "    early_stopping=EarlyStopping(monitor='val_loss')  \n",
    "\n",
    "\n",
    "    history = model_w2v.fit(xtrain_w2v, y_train,\n",
    "                        batch_size=batch_size,\n",
    "                        epochs=epochs,\n",
    "                        verbose=1,\n",
    "                        validation_split=0.2,\n",
    "                        callbacks=[tensorboard, early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "with device:\n",
    "    result_w2v = model_w2v.predict(xvalid_w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.829396911861511"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(y_val, result_w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.8459531688184458"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
