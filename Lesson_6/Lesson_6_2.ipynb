{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Урок 6. Рекуррентные нейронные сети. LSTM. GRU.#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### -- Автор: Шенк Евгений Станиславович"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Провести анализ как лучше генерировать текст по словам или символам ноутбук text_generation.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выполнение:  \n",
    "Проведено обучение модели для генерации на символах и на словах (в модели отличие только в эпохах: символы-100, слова-200 и BATCH_SIZE: символы-64, слова-16). Для разбиения на слова использован word_tokenize.  \n",
    "Результат:  \n",
    "На словах получилось в данной генерации лучше (результат внизу нотбука), но обычно и там и там результат получается средненький."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import itertools\n",
    "\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], enable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_file = '../data/evgenyi_onegin.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = sorted(set(text))\n",
    "\n",
    "char2idx = {u:i for i, u in enumerate(vocab)}\n",
    "idx2char = np.array(vocab)\n",
    "\n",
    "text_as_int = np.array([char2idx[c] for c in text])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_for_words = text.lower().replace('\\n', ' \\\\n ')\n",
    "text_in_words = word_tokenize(text_for_words, language='russian')\n",
    "text_in_words = ['\\n' if x=='\\\\n' else x for x in text_in_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_words = sorted(set(text_in_words))\n",
    "\n",
    "word2idx = {u:i for i, u in enumerate(vocab_words)}\n",
    "idx2word = np.array(vocab_words)\n",
    "\n",
    "text_in_words_as_int = np.array([word2idx[c] for c in text_in_words])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train and target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The maximum length sentence you want for a single input in characters\n",
    "seq_length = 100\n",
    "examples_per_epoch = len(text)//(seq_length+1)\n",
    "\n",
    "# Create training examples / targets\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
    "\n",
    "# For words\n",
    "w_examples_per_epoch = len(text_in_words)//(seq_length+1)\n",
    "w_char_dataset = tf.data.Dataset.from_tensor_slices(text_in_words_as_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
    "w_sequences = w_char_dataset.batch(seq_length+1, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Александр Сергеевич Пушкин\\n\\n                                Евгений Онегин\\n                          '\n"
     ]
    }
   ],
   "source": [
    "for item in sequences.take(1):\n",
    "    print(repr(''.join(idx2char[item.numpy()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'александр сергеевич пушкин \\n \\n евгений онегин \\n роман в стихах \\n \\n не мысля гордый свет забавить , \\n вниманье дружбы возлюбя , \\n хотел бы я тебе представить \\n залог достойнее тебя , \\n достойнее души прекрасной , \\n святой исполненной мечты , \\n поэзии живой и ясной , \\n высоких дум и простоты ; \\n но так и быть - рукой пристрастной \\n прими собранье пестрых глав , \\n полусмешных , полупечальных , \\n простонародных , идеальных , \\n небрежный плод моих забав , \\n бессонниц , легких вдохновений , \\n незрелых и увядших лет , \\n ума'\n"
     ]
    }
   ],
   "source": [
    "for item in w_sequences.take(1):\n",
    "    print(repr(' '.join(idx2word[item.numpy()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_input_target(chunk):\n",
    "    input_text = chunk[:-1]\n",
    "    target_text = chunk[1:]\n",
    "    return input_text, target_text\n",
    "\n",
    "dataset = sequences.map(split_input_target)\n",
    "w_dataset = w_sequences.map(split_input_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data:  'Александр Сергеевич Пушкин\\n\\n                                Евгений Онегин\\n                         '\n",
      "Target data: 'лександр Сергеевич Пушкин\\n\\n                                Евгений Онегин\\n                          '\n"
     ]
    }
   ],
   "source": [
    "for input_example, target_example in  dataset.take(1):\n",
    "    print('Input data: ', repr(''.join(idx2char[input_example.numpy()])))\n",
    "    print('Target data:', repr(''.join(idx2char[target_example.numpy()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data:  'александр сергеевич пушкин \\n \\n евгений онегин \\n роман в стихах \\n \\n не мысля гордый свет забавить , \\n вниманье дружбы возлюбя , \\n хотел бы я тебе представить \\n залог достойнее тебя , \\n достойнее души прекрасной , \\n святой исполненной мечты , \\n поэзии живой и ясной , \\n высоких дум и простоты ; \\n но так и быть - рукой пристрастной \\n прими собранье пестрых глав , \\n полусмешных , полупечальных , \\n простонародных , идеальных , \\n небрежный плод моих забав , \\n бессонниц , легких вдохновений , \\n незрелых и увядших лет , \\n'\n",
      "Target data: 'сергеевич пушкин \\n \\n евгений онегин \\n роман в стихах \\n \\n не мысля гордый свет забавить , \\n вниманье дружбы возлюбя , \\n хотел бы я тебе представить \\n залог достойнее тебя , \\n достойнее души прекрасной , \\n святой исполненной мечты , \\n поэзии живой и ясной , \\n высоких дум и простоты ; \\n но так и быть - рукой пристрастной \\n прими собранье пестрых глав , \\n полусмешных , полупечальных , \\n простонародных , идеальных , \\n небрежный плод моих забав , \\n бессонниц , легких вдохновений , \\n незрелых и увядших лет , \\n ума'\n"
     ]
    }
   ],
   "source": [
    "for input_example, target_example in  w_dataset.take(1):\n",
    "    print('Input data: ', repr(' '.join(idx2word[input_example.numpy()])))\n",
    "    print('Target data:', repr(' '.join(idx2word[target_example.numpy()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((16, 100), (16, 100)), types: (tf.int32, tf.int32)>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Buffer size to shuffle the dataset\n",
    "# (TF data is designed to work with possibly infinite sequences,\n",
    "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
    "# it maintains a buffer in which it shuffles elements).\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "w_dataset = w_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "w_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Length of the vocabulary in chars\n",
    "vocab_size = len(vocab)\n",
    "w_vocab_size = len(vocab_words)\n",
    "\n",
    "# The embedding dimension\n",
    "embedding_dim = 256\n",
    "\n",
    "# Number of RNN units\n",
    "rnn_units = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
    "                                  batch_input_shape=[batch_size, None]),\n",
    "        tf.keras.layers.GRU(rnn_units,\n",
    "                            return_sequences=True,\n",
    "                            stateful=True,\n",
    "                            recurrent_initializer='glorot_uniform'),\n",
    "        tf.keras.layers.Dense(vocab_size)\n",
    "    ])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "\n",
    "model = build_model(\n",
    "                    vocab_size=vocab_size,\n",
    "                    embedding_dim=embedding_dim,\n",
    "                    rnn_units=rnn_units,\n",
    "                    batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "\n",
    "model_for_words = build_model(\n",
    "                    vocab_size=w_vocab_size,\n",
    "                    embedding_dim=embedding_dim,\n",
    "                    rnn_units=rnn_units,\n",
    "                    batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory where the checkpoints will be saved\n",
    "char_checkpoint_dir = './training_checkpoints/char'\n",
    "word_checkpoint_dir = './training_checkpoints/word'\n",
    "# Name of the checkpoint files\n",
    "char_checkpoint_prefix = os.path.join(char_checkpoint_dir, \"ckpt_{epoch}\")\n",
    "word_checkpoint_prefix = os.path.join(word_checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "char_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=char_checkpoint_prefix,\n",
    "    save_weights_only=True)\n",
    "word_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=word_checkpoint_prefix,\n",
    "    save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(labels, logits):\n",
    "    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss=loss)\n",
    "model_for_words.compile(optimizer='adam', loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "44/44 [==============================] - 4s 98ms/step - loss: 2.3452\n",
      "Epoch 2/100\n",
      "44/44 [==============================] - 4s 98ms/step - loss: 1.6894\n",
      "Epoch 3/100\n",
      "44/44 [==============================] - 4s 96ms/step - loss: 1.4785\n",
      "Epoch 4/100\n",
      "44/44 [==============================] - 4s 96ms/step - loss: 1.3702\n",
      "Epoch 5/100\n",
      "44/44 [==============================] - 4s 97ms/step - loss: 1.3150: 0s - loss: 1.315\n",
      "Epoch 6/100\n",
      "44/44 [==============================] - 4s 98ms/step - loss: 1.2794\n",
      "Epoch 7/100\n",
      "44/44 [==============================] - 4s 96ms/step - loss: 1.2469\n",
      "Epoch 8/100\n",
      "44/44 [==============================] - 4s 97ms/step - loss: 1.2204\n",
      "Epoch 9/100\n",
      "44/44 [==============================] - 4s 98ms/step - loss: 1.2018\n",
      "Epoch 10/100\n",
      "44/44 [==============================] - 4s 97ms/step - loss: 1.1742\n",
      "Epoch 11/100\n",
      "44/44 [==============================] - 4s 96ms/step - loss: 1.1480\n",
      "Epoch 12/100\n",
      "44/44 [==============================] - 4s 100ms/step - loss: 1.1276\n",
      "Epoch 13/100\n",
      "44/44 [==============================] - 5s 105ms/step - loss: 1.1042\n",
      "Epoch 14/100\n",
      "44/44 [==============================] - 5s 106ms/step - loss: 1.0784\n",
      "Epoch 15/100\n",
      "44/44 [==============================] - 5s 103ms/step - loss: 1.0451\n",
      "Epoch 16/100\n",
      "44/44 [==============================] - 5s 104ms/step - loss: 1.0147\n",
      "Epoch 17/100\n",
      "44/44 [==============================] - 4s 100ms/step - loss: 0.9859\n",
      "Epoch 18/100\n",
      "44/44 [==============================] - 4s 100ms/step - loss: 0.9556\n",
      "Epoch 19/100\n",
      "44/44 [==============================] - 5s 105ms/step - loss: 0.9282\n",
      "Epoch 20/100\n",
      "44/44 [==============================] - 4s 102ms/step - loss: 0.8884\n",
      "Epoch 21/100\n",
      "44/44 [==============================] - 4s 100ms/step - loss: 0.8609\n",
      "Epoch 22/100\n",
      "44/44 [==============================] - 4s 101ms/step - loss: 0.8170\n",
      "Epoch 23/100\n",
      "44/44 [==============================] - 5s 104ms/step - loss: 0.7805\n",
      "Epoch 24/100\n",
      "44/44 [==============================] - 4s 101ms/step - loss: 0.7383\n",
      "Epoch 25/100\n",
      "44/44 [==============================] - 5s 107ms/step - loss: 0.7088\n",
      "Epoch 26/100\n",
      "44/44 [==============================] - 5s 103ms/step - loss: 0.6664\n",
      "Epoch 27/100\n",
      "44/44 [==============================] - 5s 103ms/step - loss: 0.6402\n",
      "Epoch 28/100\n",
      "44/44 [==============================] - 5s 103ms/step - loss: 0.5832\n",
      "Epoch 29/100\n",
      "44/44 [==============================] - 4s 101ms/step - loss: 0.5422\n",
      "Epoch 30/100\n",
      "44/44 [==============================] - 4s 99ms/step - loss: 0.5034\n",
      "Epoch 31/100\n",
      "44/44 [==============================] - 4s 98ms/step - loss: 0.4675\n",
      "Epoch 32/100\n",
      "44/44 [==============================] - 4s 98ms/step - loss: 0.4345\n",
      "Epoch 33/100\n",
      "44/44 [==============================] - 4s 97ms/step - loss: 0.4183\n",
      "Epoch 34/100\n",
      "44/44 [==============================] - 5s 103ms/step - loss: 0.3867\n",
      "Epoch 35/100\n",
      "44/44 [==============================] - 4s 101ms/step - loss: 0.3895\n",
      "Epoch 36/100\n",
      "44/44 [==============================] - 4s 101ms/step - loss: 0.3823\n",
      "Epoch 37/100\n",
      "44/44 [==============================] - 4s 100ms/step - loss: 0.3582\n",
      "Epoch 38/100\n",
      "44/44 [==============================] - 4s 101ms/step - loss: 0.3387\n",
      "Epoch 39/100\n",
      "44/44 [==============================] - 4s 101ms/step - loss: 0.3262\n",
      "Epoch 40/100\n",
      "44/44 [==============================] - 4s 100ms/step - loss: 0.3534\n",
      "Epoch 41/100\n",
      "44/44 [==============================] - 4s 101ms/step - loss: 0.3282\n",
      "Epoch 42/100\n",
      "44/44 [==============================] - 5s 102ms/step - loss: 0.3140\n",
      "Epoch 43/100\n",
      "44/44 [==============================] - 4s 101ms/step - loss: 0.3223\n",
      "Epoch 44/100\n",
      "44/44 [==============================] - 4s 101ms/step - loss: 0.3076\n",
      "Epoch 45/100\n",
      "44/44 [==============================] - 4s 102ms/step - loss: 0.3090\n",
      "Epoch 46/100\n",
      "44/44 [==============================] - 4s 102ms/step - loss: 0.2842\n",
      "Epoch 47/100\n",
      "44/44 [==============================] - 4s 101ms/step - loss: 0.3150\n",
      "Epoch 48/100\n",
      "44/44 [==============================] - 4s 101ms/step - loss: 0.2905\n",
      "Epoch 49/100\n",
      "44/44 [==============================] - 4s 102ms/step - loss: 0.2809\n",
      "Epoch 50/100\n",
      "44/44 [==============================] - 4s 100ms/step - loss: 0.2698\n",
      "Epoch 51/100\n",
      "44/44 [==============================] - 4s 101ms/step - loss: 0.2676\n",
      "Epoch 52/100\n",
      "44/44 [==============================] - 4s 100ms/step - loss: 0.2548\n",
      "Epoch 53/100\n",
      "44/44 [==============================] - 4s 101ms/step - loss: 0.2783\n",
      "Epoch 54/100\n",
      "44/44 [==============================] - 4s 102ms/step - loss: 0.2485\n",
      "Epoch 55/100\n",
      "44/44 [==============================] - 4s 102ms/step - loss: 0.2426\n",
      "Epoch 56/100\n",
      "44/44 [==============================] - 4s 101ms/step - loss: 0.2461\n",
      "Epoch 57/100\n",
      "44/44 [==============================] - 4s 100ms/step - loss: 0.2359\n",
      "Epoch 58/100\n",
      "44/44 [==============================] - 4s 102ms/step - loss: 0.2275\n",
      "Epoch 59/100\n",
      "44/44 [==============================] - 4s 101ms/step - loss: 0.2279\n",
      "Epoch 60/100\n",
      "44/44 [==============================] - 4s 102ms/step - loss: 0.2199\n",
      "Epoch 61/100\n",
      "44/44 [==============================] - 4s 100ms/step - loss: 0.2215\n",
      "Epoch 62/100\n",
      "44/44 [==============================] - 4s 101ms/step - loss: 0.2193\n",
      "Epoch 63/100\n",
      "44/44 [==============================] - 4s 101ms/step - loss: 0.2075\n",
      "Epoch 64/100\n",
      "44/44 [==============================] - 4s 101ms/step - loss: 0.2028\n",
      "Epoch 65/100\n",
      "44/44 [==============================] - 4s 100ms/step - loss: 0.2225\n",
      "Epoch 66/100\n",
      "44/44 [==============================] - 4s 101ms/step - loss: 0.2005\n",
      "Epoch 67/100\n",
      "44/44 [==============================] - 5s 102ms/step - loss: 0.2270\n",
      "Epoch 68/100\n",
      "44/44 [==============================] - 4s 100ms/step - loss: 0.1999\n",
      "Epoch 69/100\n",
      "44/44 [==============================] - 4s 102ms/step - loss: 0.1913\n",
      "Epoch 70/100\n",
      "44/44 [==============================] - 4s 102ms/step - loss: 0.2043\n",
      "Epoch 71/100\n",
      "44/44 [==============================] - 4s 101ms/step - loss: 0.1869\n",
      "Epoch 72/100\n",
      "44/44 [==============================] - 4s 100ms/step - loss: 0.1844\n",
      "Epoch 73/100\n",
      "44/44 [==============================] - 4s 102ms/step - loss: 0.2023\n",
      "Epoch 74/100\n",
      "44/44 [==============================] - 4s 102ms/step - loss: 0.1848\n",
      "Epoch 75/100\n",
      "44/44 [==============================] - 4s 100ms/step - loss: 0.1840\n",
      "Epoch 76/100\n",
      "44/44 [==============================] - 4s 102ms/step - loss: 0.1849\n",
      "Epoch 77/100\n",
      "44/44 [==============================] - 5s 102ms/step - loss: 0.1810\n",
      "Epoch 78/100\n",
      "44/44 [==============================] - 4s 101ms/step - loss: 0.1750\n",
      "Epoch 79/100\n",
      "44/44 [==============================] - 4s 101ms/step - loss: 0.2155\n",
      "Epoch 80/100\n",
      "44/44 [==============================] - 4s 101ms/step - loss: 0.1938\n",
      "Epoch 81/100\n",
      "44/44 [==============================] - 5s 103ms/step - loss: 0.1846\n",
      "Epoch 82/100\n",
      "44/44 [==============================] - 4s 101ms/step - loss: 0.1723\n",
      "Epoch 83/100\n",
      "44/44 [==============================] - 4s 101ms/step - loss: 0.1695\n",
      "Epoch 84/100\n",
      "44/44 [==============================] - 4s 102ms/step - loss: 0.1919\n",
      "Epoch 85/100\n",
      "44/44 [==============================] - 4s 102ms/step - loss: 0.1775\n",
      "Epoch 86/100\n",
      "44/44 [==============================] - 4s 100ms/step - loss: 0.1680\n",
      "Epoch 87/100\n",
      "44/44 [==============================] - 4s 101ms/step - loss: 0.1679\n",
      "Epoch 88/100\n",
      "44/44 [==============================] - 4s 101ms/step - loss: 0.1754\n",
      "Epoch 89/100\n",
      "44/44 [==============================] - 4s 101ms/step - loss: 0.2011\n",
      "Epoch 90/100\n",
      "44/44 [==============================] - 5s 102ms/step - loss: 0.2021\n",
      "Epoch 91/100\n",
      "44/44 [==============================] - 5s 103ms/step - loss: 0.1785\n",
      "Epoch 92/100\n",
      "44/44 [==============================] - 4s 101ms/step - loss: 0.1672\n",
      "Epoch 93/100\n",
      "44/44 [==============================] - 4s 101ms/step - loss: 0.1637\n",
      "Epoch 94/100\n",
      "44/44 [==============================] - 4s 101ms/step - loss: 0.1619\n",
      "Epoch 95/100\n",
      "44/44 [==============================] - 4s 101ms/step - loss: 0.1591\n",
      "Epoch 96/100\n",
      "44/44 [==============================] - 4s 101ms/step - loss: 0.1594\n",
      "Epoch 97/100\n",
      "44/44 [==============================] - 4s 101ms/step - loss: 0.1573\n",
      "Epoch 98/100\n",
      "44/44 [==============================] - 4s 102ms/step - loss: 0.2078\n",
      "Epoch 99/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44/44 [==============================] - 4s 101ms/step - loss: 0.1702\n",
      "Epoch 100/100\n",
      "44/44 [==============================] - 4s 100ms/step - loss: 0.1612\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(dataset, epochs=EPOCHS, callbacks=[char_checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "22/22 [==============================] - 3s 120ms/step - loss: 7.2841\n",
      "Epoch 2/200\n",
      "22/22 [==============================] - 3s 119ms/step - loss: 6.2822\n",
      "Epoch 3/200\n",
      "22/22 [==============================] - 3s 121ms/step - loss: 5.9630\n",
      "Epoch 4/200\n",
      "22/22 [==============================] - 3s 120ms/step - loss: 5.6641\n",
      "Epoch 5/200\n",
      "22/22 [==============================] - 3s 120ms/step - loss: 5.3848 1s\n",
      "Epoch 6/200\n",
      "22/22 [==============================] - 3s 122ms/step - loss: 5.1396\n",
      "Epoch 7/200\n",
      "22/22 [==============================] - 3s 122ms/step - loss: 4.8879\n",
      "Epoch 8/200\n",
      "22/22 [==============================] - 3s 118ms/step - loss: 4.6237\n",
      "Epoch 9/200\n",
      "22/22 [==============================] - 3s 119ms/step - loss: 4.3579 0s - loss: \n",
      "Epoch 10/200\n",
      "22/22 [==============================] - 3s 121ms/step - loss: 4.0689\n",
      "Epoch 11/200\n",
      "22/22 [==============================] - 3s 120ms/step - loss: 3.7623\n",
      "Epoch 12/200\n",
      "22/22 [==============================] - 3s 120ms/step - loss: 3.4386\n",
      "Epoch 13/200\n",
      "22/22 [==============================] - 3s 120ms/step - loss: 3.1160\n",
      "Epoch 14/200\n",
      "22/22 [==============================] - 3s 121ms/step - loss: 2.7909\n",
      "Epoch 15/200\n",
      "22/22 [==============================] - 3s 117ms/step - loss: 2.4900\n",
      "Epoch 16/200\n",
      "22/22 [==============================] - 3s 120ms/step - loss: 2.2094\n",
      "Epoch 17/200\n",
      "22/22 [==============================] - 3s 117ms/step - loss: 1.9622\n",
      "Epoch 18/200\n",
      "22/22 [==============================] - 3s 118ms/step - loss: 1.7255\n",
      "Epoch 19/200\n",
      "22/22 [==============================] - 3s 120ms/step - loss: 1.5229\n",
      "Epoch 20/200\n",
      "22/22 [==============================] - 3s 119ms/step - loss: 1.3394\n",
      "Epoch 21/200\n",
      "22/22 [==============================] - 3s 119ms/step - loss: 1.1820\n",
      "Epoch 22/200\n",
      "22/22 [==============================] - 3s 117ms/step - loss: 1.0430 0s - loss: \n",
      "Epoch 23/200\n",
      "22/22 [==============================] - 3s 120ms/step - loss: 0.9199\n",
      "Epoch 24/200\n",
      "22/22 [==============================] - 3s 120ms/step - loss: 0.8044\n",
      "Epoch 25/200\n",
      "22/22 [==============================] - 3s 119ms/step - loss: 0.7055\n",
      "Epoch 26/200\n",
      "22/22 [==============================] - 3s 118ms/step - loss: 0.6127 0s - l\n",
      "Epoch 27/200\n",
      "22/22 [==============================] - 3s 118ms/step - loss: 0.5364\n",
      "Epoch 28/200\n",
      "22/22 [==============================] - 3s 116ms/step - loss: 0.4698\n",
      "Epoch 29/200\n",
      "22/22 [==============================] - 3s 117ms/step - loss: 0.4077\n",
      "Epoch 30/200\n",
      "22/22 [==============================] - 3s 121ms/step - loss: 0.3595\n",
      "Epoch 31/200\n",
      "22/22 [==============================] - 3s 119ms/step - loss: 0.3138\n",
      "Epoch 32/200\n",
      "22/22 [==============================] - 3s 116ms/step - loss: 0.2831\n",
      "Epoch 33/200\n",
      "22/22 [==============================] - 3s 118ms/step - loss: 0.2539\n",
      "Epoch 34/200\n",
      "22/22 [==============================] - 3s 119ms/step - loss: 0.2262\n",
      "Epoch 35/200\n",
      "22/22 [==============================] - 3s 120ms/step - loss: 0.2097\n",
      "Epoch 36/200\n",
      "22/22 [==============================] - 3s 125ms/step - loss: 0.1889\n",
      "Epoch 37/200\n",
      "22/22 [==============================] - 3s 122ms/step - loss: 0.1739\n",
      "Epoch 38/200\n",
      "22/22 [==============================] - 3s 123ms/step - loss: 0.1618\n",
      "Epoch 39/200\n",
      "22/22 [==============================] - 3s 120ms/step - loss: 0.1542\n",
      "Epoch 40/200\n",
      "22/22 [==============================] - 3s 120ms/step - loss: 0.1451 0s - loss: 0.1\n",
      "Epoch 41/200\n",
      "22/22 [==============================] - 3s 121ms/step - loss: 0.1359\n",
      "Epoch 42/200\n",
      "22/22 [==============================] - 3s 120ms/step - loss: 0.1337\n",
      "Epoch 43/200\n",
      "22/22 [==============================] - 3s 119ms/step - loss: 0.1216\n",
      "Epoch 44/200\n",
      "22/22 [==============================] - 3s 121ms/step - loss: 0.1186\n",
      "Epoch 45/200\n",
      "22/22 [==============================] - 3s 119ms/step - loss: 0.1110\n",
      "Epoch 46/200\n",
      "22/22 [==============================] - 3s 119ms/step - loss: 0.1077\n",
      "Epoch 47/200\n",
      "22/22 [==============================] - 3s 119ms/step - loss: 0.1060\n",
      "Epoch 48/200\n",
      "22/22 [==============================] - 3s 121ms/step - loss: 0.1015 0s - loss: \n",
      "Epoch 49/200\n",
      "22/22 [==============================] - 3s 118ms/step - loss: 0.0979\n",
      "Epoch 50/200\n",
      "22/22 [==============================] - 3s 120ms/step - loss: 0.0940\n",
      "Epoch 51/200\n",
      "22/22 [==============================] - 3s 123ms/step - loss: 0.0917\n",
      "Epoch 52/200\n",
      "22/22 [==============================] - 3s 119ms/step - loss: 0.0933\n",
      "Epoch 53/200\n",
      "22/22 [==============================] - 3s 119ms/step - loss: 0.0876\n",
      "Epoch 54/200\n",
      "22/22 [==============================] - 3s 119ms/step - loss: 0.0804\n",
      "Epoch 55/200\n",
      "22/22 [==============================] - 3s 122ms/step - loss: 0.0792\n",
      "Epoch 56/200\n",
      "22/22 [==============================] - 3s 120ms/step - loss: 0.0758\n",
      "Epoch 57/200\n",
      "22/22 [==============================] - 3s 121ms/step - loss: 0.0765\n",
      "Epoch 58/200\n",
      "22/22 [==============================] - 3s 119ms/step - loss: 0.0748 0s - loss: \n",
      "Epoch 59/200\n",
      "22/22 [==============================] - 3s 119ms/step - loss: 0.0732 1s -\n",
      "Epoch 60/200\n",
      "22/22 [==============================] - 3s 119ms/step - loss: 0.0696 0s - los\n",
      "Epoch 61/200\n",
      "22/22 [==============================] - 3s 116ms/step - loss: 0.0695\n",
      "Epoch 62/200\n",
      "22/22 [==============================] - 3s 120ms/step - loss: 0.0653\n",
      "Epoch 63/200\n",
      "22/22 [==============================] - 3s 118ms/step - loss: 0.0665\n",
      "Epoch 64/200\n",
      "22/22 [==============================] - 3s 119ms/step - loss: 0.0648\n",
      "Epoch 65/200\n",
      "22/22 [==============================] - 3s 118ms/step - loss: 0.0641\n",
      "Epoch 66/200\n",
      "22/22 [==============================] - 3s 120ms/step - loss: 0.0632\n",
      "Epoch 67/200\n",
      "22/22 [==============================] - 3s 121ms/step - loss: 0.0599\n",
      "Epoch 68/200\n",
      "22/22 [==============================] - 3s 119ms/step - loss: 0.0603\n",
      "Epoch 69/200\n",
      "22/22 [==============================] - 3s 121ms/step - loss: 0.0604\n",
      "Epoch 70/200\n",
      "22/22 [==============================] - 3s 120ms/step - loss: 0.0582\n",
      "Epoch 71/200\n",
      "22/22 [==============================] - 3s 118ms/step - loss: 0.0558\n",
      "Epoch 72/200\n",
      "22/22 [==============================] - 3s 120ms/step - loss: 0.0562\n",
      "Epoch 73/200\n",
      "22/22 [==============================] - 3s 120ms/step - loss: 0.0534\n",
      "Epoch 74/200\n",
      "22/22 [==============================] - 3s 119ms/step - loss: 0.0533 0s - loss: 0.\n",
      "Epoch 75/200\n",
      "22/22 [==============================] - 3s 120ms/step - loss: 0.0533\n",
      "Epoch 76/200\n",
      "22/22 [==============================] - 3s 121ms/step - loss: 0.0524\n",
      "Epoch 77/200\n",
      "22/22 [==============================] - 3s 121ms/step - loss: 0.0524\n",
      "Epoch 78/200\n",
      "22/22 [==============================] - 3s 122ms/step - loss: 0.0489\n",
      "Epoch 79/200\n",
      "22/22 [==============================] - 3s 120ms/step - loss: 0.0496\n",
      "Epoch 80/200\n",
      "22/22 [==============================] - 3s 121ms/step - loss: 0.0487\n",
      "Epoch 81/200\n",
      "22/22 [==============================] - 3s 120ms/step - loss: 0.0464\n",
      "Epoch 82/200\n",
      "22/22 [==============================] - 3s 122ms/step - loss: 0.0493\n",
      "Epoch 83/200\n",
      "22/22 [==============================] - 3s 120ms/step - loss: 0.0507\n",
      "Epoch 84/200\n",
      "22/22 [==============================] - 3s 120ms/step - loss: 0.0476\n",
      "Epoch 85/200\n",
      "22/22 [==============================] - 3s 118ms/step - loss: 0.0444\n",
      "Epoch 86/200\n",
      "22/22 [==============================] - 3s 122ms/step - loss: 0.0448\n",
      "Epoch 87/200\n",
      "22/22 [==============================] - 3s 119ms/step - loss: 0.0473\n",
      "Epoch 88/200\n",
      "22/22 [==============================] - 3s 121ms/step - loss: 0.0442\n",
      "Epoch 89/200\n",
      "22/22 [==============================] - 3s 119ms/step - loss: 0.0452\n",
      "Epoch 90/200\n",
      "22/22 [==============================] - 3s 121ms/step - loss: 0.0435\n",
      "Epoch 91/200\n",
      "22/22 [==============================] - 3s 120ms/step - loss: 0.0431\n",
      "Epoch 92/200\n",
      "22/22 [==============================] - 3s 120ms/step - loss: 0.0438\n",
      "Epoch 93/200\n",
      "22/22 [==============================] - 3s 120ms/step - loss: 0.0438\n",
      "Epoch 94/200\n",
      "22/22 [==============================] - 3s 120ms/step - loss: 0.0435\n",
      "Epoch 95/200\n",
      "22/22 [==============================] - 3s 119ms/step - loss: 0.0416\n",
      "Epoch 96/200\n",
      "22/22 [==============================] - 3s 122ms/step - loss: 0.0420 0s - loss: 0.\n",
      "Epoch 97/200\n",
      "22/22 [==============================] - 3s 120ms/step - loss: 0.0394\n",
      "Epoch 98/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/22 [==============================] - 3s 120ms/step - loss: 0.0403\n",
      "Epoch 99/200\n",
      "22/22 [==============================] - 3s 121ms/step - loss: 0.0403\n",
      "Epoch 100/200\n",
      "22/22 [==============================] - 3s 121ms/step - loss: 0.0396\n",
      "Epoch 101/200\n",
      "22/22 [==============================] - 3s 120ms/step - loss: 0.0397\n",
      "Epoch 102/200\n",
      "22/22 [==============================] - 3s 120ms/step - loss: 0.0394\n",
      "Epoch 103/200\n",
      "22/22 [==============================] - 3s 119ms/step - loss: 0.0400\n",
      "Epoch 104/200\n",
      "22/22 [==============================] - 3s 119ms/step - loss: 0.0403\n",
      "Epoch 105/200\n",
      "22/22 [==============================] - 3s 120ms/step - loss: 0.0383\n",
      "Epoch 106/200\n",
      "22/22 [==============================] - 3s 118ms/step - loss: 0.0403\n",
      "Epoch 107/200\n",
      "22/22 [==============================] - 3s 120ms/step - loss: 0.0387\n",
      "Epoch 108/200\n",
      "22/22 [==============================] - 3s 118ms/step - loss: 0.0383\n",
      "Epoch 109/200\n",
      "22/22 [==============================] - 3s 120ms/step - loss: 0.0368\n",
      "Epoch 110/200\n",
      "22/22 [==============================] - 3s 121ms/step - loss: 0.0380\n",
      "Epoch 111/200\n",
      "22/22 [==============================] - 3s 119ms/step - loss: 0.0368\n",
      "Epoch 112/200\n",
      "22/22 [==============================] - 3s 122ms/step - loss: 0.0392\n",
      "Epoch 113/200\n",
      "22/22 [==============================] - 3s 119ms/step - loss: 0.0393\n",
      "Epoch 114/200\n",
      "22/22 [==============================] - 3s 120ms/step - loss: 0.0369\n",
      "Epoch 115/200\n",
      "22/22 [==============================] - 3s 120ms/step - loss: 0.0370\n",
      "Epoch 116/200\n",
      "22/22 [==============================] - 3s 120ms/step - loss: 0.0397\n",
      "Epoch 117/200\n",
      "22/22 [==============================] - 3s 119ms/step - loss: 0.0371\n",
      "Epoch 118/200\n",
      "22/22 [==============================] - 3s 120ms/step - loss: 0.0392\n",
      "Epoch 119/200\n",
      "22/22 [==============================] - 3s 121ms/step - loss: 0.0378\n",
      "Epoch 120/200\n",
      "22/22 [==============================] - 3s 119ms/step - loss: 0.0380\n",
      "Epoch 121/200\n",
      "22/22 [==============================] - 3s 121ms/step - loss: 0.0376\n",
      "Epoch 122/200\n",
      "22/22 [==============================] - 3s 120ms/step - loss: 0.0398\n",
      "Epoch 123/200\n",
      "22/22 [==============================] - 3s 120ms/step - loss: 0.0416\n",
      "Epoch 124/200\n",
      "22/22 [==============================] - 3s 121ms/step - loss: 0.0460\n",
      "Epoch 125/200\n",
      "22/22 [==============================] - 3s 119ms/step - loss: 0.0461\n",
      "Epoch 126/200\n",
      "22/22 [==============================] - 3s 119ms/step - loss: 0.0471\n",
      "Epoch 127/200\n",
      "22/22 [==============================] - 3s 121ms/step - loss: 0.0449\n",
      "Epoch 128/200\n",
      "22/22 [==============================] - 3s 121ms/step - loss: 0.0461\n",
      "Epoch 129/200\n",
      "22/22 [==============================] - 3s 120ms/step - loss: 0.0472\n",
      "Epoch 130/200\n",
      "22/22 [==============================] - 3s 119ms/step - loss: 0.0472\n",
      "Epoch 131/200\n",
      "22/22 [==============================] - 3s 121ms/step - loss: 0.0409\n",
      "Epoch 132/200\n",
      "22/22 [==============================] - 3s 120ms/step - loss: 0.0409\n",
      "Epoch 133/200\n",
      "22/22 [==============================] - 3s 121ms/step - loss: 0.0455\n",
      "Epoch 134/200\n",
      "22/22 [==============================] - 3s 120ms/step - loss: 0.0464\n",
      "Epoch 135/200\n",
      "22/22 [==============================] - 3s 119ms/step - loss: 0.0451\n",
      "Epoch 136/200\n",
      "22/22 [==============================] - 3s 120ms/step - loss: 0.0428\n",
      "Epoch 137/200\n",
      "22/22 [==============================] - 3s 119ms/step - loss: 0.0405 0s - loss: 0.04\n",
      "Epoch 138/200\n",
      "22/22 [==============================] - 3s 120ms/step - loss: 0.0375 1s\n",
      "Epoch 139/200\n",
      "22/22 [==============================] - 3s 122ms/step - loss: 0.0369\n",
      "Epoch 140/200\n",
      "22/22 [==============================] - 3s 120ms/step - loss: 0.0344\n",
      "Epoch 141/200\n",
      "22/22 [==============================] - 3s 121ms/step - loss: 0.0361\n",
      "Epoch 142/200\n",
      "22/22 [==============================] - 3s 123ms/step - loss: 0.0375\n",
      "Epoch 143/200\n",
      "22/22 [==============================] - 3s 121ms/step - loss: 0.0349\n",
      "Epoch 144/200\n",
      "22/22 [==============================] - 3s 121ms/step - loss: 0.0342\n",
      "Epoch 145/200\n",
      "22/22 [==============================] - 3s 121ms/step - loss: 0.0353\n",
      "Epoch 146/200\n",
      "22/22 [==============================] - 3s 120ms/step - loss: 0.0339\n",
      "Epoch 147/200\n",
      "22/22 [==============================] - 3s 120ms/step - loss: 0.0344\n",
      "Epoch 148/200\n",
      "22/22 [==============================] - 3s 121ms/step - loss: 0.0339\n",
      "Epoch 149/200\n",
      "22/22 [==============================] - 3s 120ms/step - loss: 0.0334\n",
      "Epoch 150/200\n",
      "22/22 [==============================] - 3s 120ms/step - loss: 0.0360\n",
      "Epoch 151/200\n",
      "22/22 [==============================] - 3s 120ms/step - loss: 0.0351 0s - loss\n",
      "Epoch 152/200\n",
      "22/22 [==============================] - 3s 121ms/step - loss: 0.0342\n",
      "Epoch 153/200\n",
      "22/22 [==============================] - 3s 121ms/step - loss: 0.0357\n",
      "Epoch 154/200\n",
      "22/22 [==============================] - 3s 120ms/step - loss: 0.0342\n",
      "Epoch 155/200\n",
      "22/22 [==============================] - 3s 120ms/step - loss: 0.0329\n",
      "Epoch 156/200\n",
      "22/22 [==============================] - 3s 120ms/step - loss: 0.0346\n",
      "Epoch 157/200\n",
      "22/22 [==============================] - 3s 120ms/step - loss: 0.0338\n",
      "Epoch 158/200\n",
      "22/22 [==============================] - 3s 118ms/step - loss: 0.0351\n",
      "Epoch 159/200\n",
      "22/22 [==============================] - 3s 122ms/step - loss: 0.0353\n",
      "Epoch 160/200\n",
      "22/22 [==============================] - 3s 120ms/step - loss: 0.0327\n",
      "Epoch 161/200\n",
      "22/22 [==============================] - 3s 120ms/step - loss: 0.0348\n",
      "Epoch 162/200\n",
      "22/22 [==============================] - 3s 120ms/step - loss: 0.0339\n",
      "Epoch 163/200\n",
      "22/22 [==============================] - 3s 121ms/step - loss: 0.0362 1s\n",
      "Epoch 164/200\n",
      "22/22 [==============================] - 3s 120ms/step - loss: 0.0376\n",
      "Epoch 165/200\n",
      "22/22 [==============================] - 3s 119ms/step - loss: 0.0331\n",
      "Epoch 166/200\n",
      "22/22 [==============================] - 3s 121ms/step - loss: 0.0333\n",
      "Epoch 167/200\n",
      "22/22 [==============================] - 3s 120ms/step - loss: 0.0346\n",
      "Epoch 168/200\n",
      "22/22 [==============================] - 3s 119ms/step - loss: 0.0351\n",
      "Epoch 169/200\n",
      "22/22 [==============================] - 3s 121ms/step - loss: 0.0358\n",
      "Epoch 170/200\n",
      "22/22 [==============================] - 3s 121ms/step - loss: 0.0333\n",
      "Epoch 171/200\n",
      "22/22 [==============================] - 3s 121ms/step - loss: 0.0352\n",
      "Epoch 172/200\n",
      "22/22 [==============================] - 3s 119ms/step - loss: 0.0348 0s - lo\n",
      "Epoch 173/200\n",
      "22/22 [==============================] - 3s 120ms/step - loss: 0.0358\n",
      "Epoch 174/200\n",
      "22/22 [==============================] - 3s 121ms/step - loss: 0.0358\n",
      "Epoch 175/200\n",
      "22/22 [==============================] - 3s 121ms/step - loss: 0.0372\n",
      "Epoch 176/200\n",
      "22/22 [==============================] - 3s 120ms/step - loss: 0.0365\n",
      "Epoch 177/200\n",
      "22/22 [==============================] - 3s 120ms/step - loss: 0.0357\n",
      "Epoch 178/200\n",
      "22/22 [==============================] - 3s 120ms/step - loss: 0.0379\n",
      "Epoch 179/200\n",
      "22/22 [==============================] - 3s 120ms/step - loss: 0.0388\n",
      "Epoch 180/200\n",
      "22/22 [==============================] - 3s 121ms/step - loss: 0.0431 0s - lo\n",
      "Epoch 181/200\n",
      "22/22 [==============================] - 3s 121ms/step - loss: 0.0442\n",
      "Epoch 182/200\n",
      "22/22 [==============================] - 3s 120ms/step - loss: 0.0440\n",
      "Epoch 183/200\n",
      "22/22 [==============================] - 3s 119ms/step - loss: 0.0486\n",
      "Epoch 184/200\n",
      "22/22 [==============================] - 3s 121ms/step - loss: 0.0502\n",
      "Epoch 185/200\n",
      "22/22 [==============================] - 3s 121ms/step - loss: 0.0609\n",
      "Epoch 186/200\n",
      "22/22 [==============================] - 3s 121ms/step - loss: 0.0676\n",
      "Epoch 187/200\n",
      "22/22 [==============================] - 3s 120ms/step - loss: 0.0788\n",
      "Epoch 188/200\n",
      "22/22 [==============================] - 3s 121ms/step - loss: 0.0899\n",
      "Epoch 189/200\n",
      "22/22 [==============================] - 3s 120ms/step - loss: 0.0958 0s - los\n",
      "Epoch 190/200\n",
      "22/22 [==============================] - 3s 121ms/step - loss: 0.0984\n",
      "Epoch 191/200\n",
      "22/22 [==============================] - 3s 121ms/step - loss: 0.0879\n",
      "Epoch 192/200\n",
      "22/22 [==============================] - 3s 119ms/step - loss: 0.0805\n",
      "Epoch 193/200\n",
      "22/22 [==============================] - 3s 120ms/step - loss: 0.0625 0s - loss: 0.060 - ETA: 0s - los\n",
      "Epoch 194/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/22 [==============================] - 3s 117ms/step - loss: 0.0537\n",
      "Epoch 195/200\n",
      "22/22 [==============================] - 3s 120ms/step - loss: 0.0459\n",
      "Epoch 196/200\n",
      "22/22 [==============================] - 3s 121ms/step - loss: 0.0413\n",
      "Epoch 197/200\n",
      "22/22 [==============================] - 3s 120ms/step - loss: 0.0360\n",
      "Epoch 198/200\n",
      "22/22 [==============================] - 3s 120ms/step - loss: 0.0352\n",
      "Epoch 199/200\n",
      "22/22 [==============================] - 3s 119ms/step - loss: 0.0346\n",
      "Epoch 200/200\n",
      "22/22 [==============================] - 3s 120ms/step - loss: 0.0362\n"
     ]
    }
   ],
   "source": [
    "history_for_words = model_for_words.fit(w_dataset, epochs=EPOCHS, callbacks=[word_checkpoint_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, start_string, num_generate = 500, unit='char'):  # word\n",
    "    # Evaluation step (generating text using the learned model)\n",
    "\n",
    "    # Number of characters to generate\n",
    "    num_generate = num_generate\n",
    "\n",
    "    # Converting our start string to numbers (vectorizing)\n",
    "    if unit=='char':\n",
    "        input_eval = [char2idx[s] for s in start_string]\n",
    "    elif unit=='word':\n",
    "        start_string_words = [w for w in start_string.lower().split(' ') if w.strip() != '' or w == '\\n']\n",
    "        input_eval = [word2idx[s] for s in start_string_words]\n",
    "    else:\n",
    "        print(\"Unit != 'char' or 'word'\")\n",
    "        return \"\"\n",
    "    \n",
    "    input_eval = tf.expand_dims(input_eval, 0)\n",
    "\n",
    "    # Empty string to store our results\n",
    "    text_generated = []\n",
    "\n",
    "    # Low temperature results in more predictable text.\n",
    "    # Higher temperature results in more surprising text.\n",
    "    # Experiment to find the best setting.\n",
    "    temperature = 1\n",
    "\n",
    "    # Here batch size == 1\n",
    "    model.reset_states()\n",
    "    for i in range(num_generate):\n",
    "        predictions = model(input_eval)\n",
    "        predictions = tf.squeeze(predictions, 0)\n",
    "        # using a categorical distribution to predict the character returned by the model\n",
    "        predictions = predictions / temperature\n",
    "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1, 0].numpy()\n",
    "\n",
    "        # Pass the predicted character as the next input to the model\n",
    "        # along with the previous hidden state\n",
    "        input_eval = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "        if unit=='char':\n",
    "            text_generated.append(idx2char[predicted_id])\n",
    "        elif unit=='word':\n",
    "            text_generated.append(idx2word[predicted_id])          \n",
    "        else:\n",
    "            print(\"Unit != 'char' or 'word'\")\n",
    "            return \"\"\n",
    "    \n",
    "    if unit=='word':\n",
    "        return (start_string + ' '.join(text_generated))          \n",
    "    \n",
    "    return (start_string + ''.join(text_generated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./training_checkpoints/char\\\\ckpt_100'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.train.latest_checkpoint(char_checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
    "\n",
    "model.load_weights(tf.train.latest_checkpoint(char_checkpoint_dir))\n",
    "\n",
    "model.build(tf.TensorShape([1, None]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Уж месяц был общий приезжатв.\n",
      "\n",
      "                                      XI\n",
      "\n",
      "                        Да, может быть, едет на изумленьем\n",
      "                        О ножки, полно за столов                    Ответа которой окрыль;\n",
      "                        Вытомой на рукой,\n",
      "                        Услышу ль вновь я ваши хоры?\n",
      "                        Взгляну на дев, как садин, А нет.\n",
      "\n",
      "                                    XXXII\n",
      "\n",
      "                        И что ж на самом Пера его,\n",
      "                        Подруге юности мят\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(model, start_string=u\"Уж месяц был \", num_generate=500, unit='char'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Уж месяц был и свотоено в волных снежен  Моствы, вестей и несет;  \n",
    "                        Оно своей игрой и пестрой Канет  \n",
    "                        И быстрый може мечтанье  \n",
    "                                     Свободно дома принимает,  \n",
    "                        Трядов наивом стрелой.  \n",
    "                        Вас голову залоглестится она!  \n",
    "                        И вот однако ж, проклиная),  \n",
    "                        Всегда нахмурен, молчалив,  \n",
    "                        Не знаю, правопистью порой  \n",
    "                       Или   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                        Уж месяц был общий приезжатв.  \n",
    "\n",
    "                                      XI  \n",
    "\n",
    "                        Да, может быть, едет на изумленьем  \n",
    "                        О ножки, полно за столов                    Ответа которой окрыль;  \n",
    "                        Вытомой на рукой,  \n",
    "                        Услышу ль вновь я ваши хоры?  \n",
    "                        Взгляну на дев, как садин, А нет.  \n",
    "\n",
    "                                    XXXII  \n",
    "\n",
    "                        И что ж на самом Пера его,  \n",
    "                        Подруге юности мят  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./training_checkpoints/word\\\\ckpt_200'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.train.latest_checkpoint(word_checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_for_words = build_model(w_vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
    "\n",
    "model_for_words.load_weights(tf.train.latest_checkpoint(word_checkpoint_dir))\n",
    "\n",
    "model_for_words.build(tf.TensorShape([1, None]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Уж месяц был печальное устах приношенье дамам веселых долго , что пора без душою ? \n",
      " кто , лучше , \n",
      " блеснул мороз . и рады мы \n",
      " проказам матушки зимы . \n",
      " не радо ей лишь сердце тани . \n",
      " нейдет она зиму встречать , \n",
      " морозной пылью подышать \n",
      " и первым\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(model_for_words, start_string=u\"Уж месяц был \", num_generate=50, unit='word'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Уж месяц был темноте хозяйский наши раз плеча же скорей  \n",
    " марает он ответ учтивый .  \n",
    " никто б не мог ее прекрасной  \n",
    " назвать ; но с головы до ног  \n",
    " никто бы в ней найти не мог  \n",
    " того , что модой самовластной  \n",
    " в высоком лондонском кругу  \n",
    " зовется vulgаr  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Уж месяц был печальное устах приношенье дамам веселых долго , что пора без душою ?  \n",
    " кто , лучше ,  \n",
    " блеснул мороз . и рады мы  \n",
    " проказам матушки зимы .  \n",
    " не радо ей лишь сердце тани .  \n",
    " нейдет она зиму встречать ,  \n",
    " морозной пылью подышать  \n",
    " и первым  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
